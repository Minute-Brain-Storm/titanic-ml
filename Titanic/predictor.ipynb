{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, multiprocessing, csv, copy\n",
    "\n",
    "import sklearn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "INPUT_PATH = os.path.join(\".\", \"CSVs\", \"inputs\")\n",
    "OUTPUT_PATH = os.path.join(\".\", \"CSVs\", \"outputs\")\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from the CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join(INPUT_PATH, \"test.csv\"))\n",
    "train = pd.read_csv(os.path.join(INPUT_PATH, \"train.csv\"))\n",
    "\n",
    "train_X = train.drop(axis = 1, columns = \"Survived\")\n",
    "train_y = train[\"Survived\"]\n",
    "marker = len(train_X)\n",
    "\n",
    "dataset = pd.concat([train_X, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribs):\n",
    "        self.attribs = attribs\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns = self.attribs)\n",
    "    def fit(self, X, y=None):\n",
    "        return X\n",
    "    def fit_transform(self, X, y=None):\n",
    "        r_val = self.transform(X)\n",
    "        self.cols = r_val.columns\n",
    "        return r_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeEstimator(BaseEstimator, TransformerMixin):\n",
    "    def transform(self, X):\n",
    "        return self\n",
    "    def fit(self, X, y=None):\n",
    "        valid = X.dropna(subset=['Age'], inplace = False)\n",
    "        missing = X[X.isnull()['Age']]\n",
    "        valid_X = valid.drop(columns=['Age'], inplace=False)\n",
    "\n",
    "        self.regr = Lasso(alpha = 0.1).fit(valid_X, valid[\"Age\"])\n",
    "        X.loc[X.isnull()['Age'], 'Age'] = self.regr.predict(missing.drop(columns = ['Age']))\n",
    "        return X\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Filler(BaseEstimator, TransformerMixin):\n",
    "    def transform(self, X):\n",
    "        return self\n",
    "    def fit(self, X, y=None):\n",
    "        for x in X:\n",
    "            if (x != 'Age') & (x != 'Sex') & (x != 'Embarked'):\n",
    "                median = X[x].median()\n",
    "                X[x].fillna(median, inplace=True)\n",
    "        return X\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(BaseEstimator, TransformerMixin):\n",
    "    def transform(self, X):\n",
    "        df = pd.DataFrame({'class_f':[], 'class_s':[], 'class_t':[],\n",
    "                          'emb_s':[], 'emb_c':[], 'emb_q':[],\n",
    "                          'officer':[], 'royalty':[], 'mr':[], 'mrs':[], 'miss':[], 'master':[]})\n",
    "        class_dict = {1:'class_f', 2:'class_s', 3:'class_t'}\n",
    "        emb_dict = {'S':'emb_s', 'C':'emb_c', 'Q':'emb_q'}\n",
    "        title_dict = {\n",
    "            'capt':'officer',\n",
    "            'col':'officer',\n",
    "            'major':'officer',\n",
    "            'jonkheer':'royalty',\n",
    "            'don':'royalty',\n",
    "            'sir':'royalty',\n",
    "            'dr':'officer',\n",
    "            'rev':'officer',\n",
    "            'the countess':'royalty',\n",
    "            'mme':'mrs',\n",
    "            'mlle':'miss',\n",
    "            'ms':'mrs',\n",
    "            'mr':'mr',\n",
    "            'mrs':'mrs',\n",
    "            'miss':'miss',\n",
    "            'master':'master',\n",
    "            'lady':'royalty'\n",
    "        } #dict taken from https://www.ahmedbesbes.com/blog/kaggle-titanic-competition\n",
    "        \n",
    "        for i in X.index:\n",
    "            class_key = X.at[i, 'Pclass']\n",
    "            emb_key = X.at[i, 'Embarked']\n",
    "            \n",
    "            if class_key in class_dict:\n",
    "                df.at[i, class_dict[class_key]] = 1\n",
    "            if emb_key in emb_dict:\n",
    "                df.at[i, emb_dict[emb_key]] = 1\n",
    "            \n",
    "            name = X.at[i, 'Name'].lower()\n",
    "            for key in title_dict.keys():\n",
    "                if name.find(key) > -1:\n",
    "                    df.at[i, title_dict[key]] = 1\n",
    "                    break\n",
    "        \n",
    "        X = X.mask(X == 'male', 0)\n",
    "        X = X.mask(X == 'female', 1)\n",
    "                \n",
    "        df.fillna(0, inplace = True)\n",
    "        return pd.concat((df, X.drop(columns = ['Pclass', 'Embarked', 'Name'])), join = 'inner', axis = 1)\n",
    "    def fit(self, X, y=None):\n",
    "        return X\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FamilyCombiner(BaseEstimator, TransformerMixin):\n",
    "    def transform(self, X):\n",
    "        fam = []\n",
    "        for i in X.index:\n",
    "            num = X.at[i, 'SibSp'] +  X.at[i, 'Parch']\n",
    "            fam.append(num)\n",
    "        X['family_size'] = fam\n",
    "        X.drop(columns = ['SibSp', 'Parch'], inplace = True)\n",
    "        return X\n",
    "    def fit(self, X, y=None):\n",
    "        return X\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "drop_attribs = ['Cabin', 'PassengerId', 'Ticket'] #need to be dropped before the age estimator, aren't good estimators\n",
    "drop_attribs2 = ['royalty', 'mrs', 'master', 'officer', 'emb_s', 'emb_c', 'emb_q'] #according to a random forest classifier, these are the least important features\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('encoder', Encoder()),\n",
    "    ('family_comb', FamilyCombiner()),\n",
    "    ('dropper', AttributeDropper(drop_attribs)),\n",
    "    ('filler', Filler()),\n",
    "    ('age_est', AgeEstimator()),\n",
    "    ('addtl_dropper', AttributeDropper(drop_attribs2)), #comment out this line and rerun the cell with the mathplot to see the importances of these features, also uncomment the line commented out in the cell below this \n",
    "    ('scaler', MinMaxScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep = pipeline.fit_transform(dataset)\n",
    "cols = pipeline.named_steps['addtl_dropper'].cols #comment here\n",
    "#cols = pipeline.named_steps['dropper'].cols #uncomment here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the prepared data back into two seperate ndarrays, the test and training vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_prep = data_prep[:marker]\n",
    "test_prep = data_prep[marker:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Regressor\n",
    "\n",
    "param_grid = [{\n",
    "    'n_estimators': [130, 150, 160, 170, 180, 200],\n",
    "    'max_depth': [6, 7, 8, 9, 10],\n",
    "    \n",
    "}]\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "grid_search = GridSearchCV(forest_clf, param_grid, cv=5,\n",
    "                           scoring='f1',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(train_X_prep, train_y)\n",
    "random_forest = grid_search.best_estimator_\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "forest_vals = random_forest.predict(test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vals = random_forest.feature_importances_\n",
    "plt.xticks(rotation=45)\n",
    "plt.bar(range(1, len(vals) + 1), vals, tick_label = cols,\n",
    "        width = 0.5, color = ['blue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_prob = random_forest.predict_proba(test_prep)\n",
    "print(rf_prob[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Classifier\n",
    "svc = SVC(probability = True, kernel = 'rbf')\n",
    "svc.fit(train_X_prep, train_y)\n",
    "svc_vals = svc.predict(test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_prob = svc.predict_proba(test_prep)\n",
    "print(svc_prob[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(estimators = [(\"rf\", random_forest), (\"svc\", svc)], voting = \"soft\")\n",
    "voting_clf.fit(train_X_prep, train_y)\n",
    "\n",
    "voting_vals = voting_clf.predict(test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using adaptive boosting with the SVC (Unsucsessful)\n",
    "param_grid = [\n",
    "    {'n_estimators': [90, 100, 110, 120], 'max_features': [4, 5, 6, 7, 8]}\n",
    "]\n",
    "\n",
    "adaboost_svc = AdaBoostClassifier(SVC(probability = True), random_state=RANDOM_STATE)\n",
    "\n",
    "ABCsvc_search = GridSearchCV(adaboost_svc, param_grid, cv=5,\n",
    "                           scoring='neg_root_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "ABCsvc_search.fit(train_X_prep, train_y)\n",
    "ABCsvc_vals = ABCsvc_search.predict(test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGDboosting\n",
    "import xgboost as xgb\n",
    "\n",
    "#params = {\n",
    "#    'max_depth' : [3, 4, 5, 6, 7, 8], 'eta' : [.05, .06, .075, .1], 'gamma': [.3, .35, .4, .45, .5], #grid search to get a feel for the best params\n",
    "#    'objective' : ['binary:hinge'], 'eval_metric' : ['logloss']\n",
    "#}\n",
    "\n",
    "\n",
    "#XGBsearch_CV = GridSearchCV(estimator=xgb.XGBRegressor(), param_grid = params,\n",
    "#                            cv=5)\n",
    "\n",
    "#grid_result = XGBsearch_CV.fit(train_X_prep, train_y)\n",
    "#print(grid_result.best_params_)\n",
    "\n",
    "dtrain = xgb.DMatrix(train_X_prep, label=train_y)\n",
    "dtest = xgb.DMatrix(test_prep)\n",
    "\n",
    "\n",
    "param = {'max_depth': 6, 'eta': 0.04, 'objective': 'binary:hinge'}\n",
    "XGB_est = xgb.train(param, dtrain, 200)\n",
    "\n",
    "# make prediction\n",
    "XGB_vals = XGB_est.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voting between xgb and svc\n",
    "voting_vals2 = []\n",
    "for i in range(418):\n",
    "    voting_vals2.append(round((svc_prob[i, 1] + XGB_vals[i]) / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(OUTPUT_PATH, \"XGB.csv\")\n",
    "vals = XGB_vals\n",
    "\n",
    "with open(filename, 'w', newline='') as csvfile:\n",
    "    # creating a csv writer object  \n",
    "    csvwriter = csv.writer(csvfile)\n",
    "\n",
    "    # writing the fields  \n",
    "    csvwriter.writerow([\"PassengerId\",\"Survived\"])\n",
    "    for num in range(418):\n",
    "        csvwriter.writerow([num+892, int(vals[num])])\n",
    "\n",
    "'''\n",
    "scores: (accuracy)\n",
    "    SGD classifier 0.73205\n",
    "    Random Forest 0.76555\n",
    "    SVC 0.78708\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "param_grid = [{'n_estimators': [100, 150, 200, 250, 300], 'max_features': [2, 3, 4, 5, 6, 7, 8]}]\n",
    "class AgeEstimatorTest(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, params = param_grid):\n",
    "        self.params = params\n",
    "    def transform(self, X):\n",
    "        return self\n",
    "    def fit(self, X, y=None):\n",
    "        temp = X.drop(columns=['Age'])\n",
    "        self.regr = Lasso(alpha = 0.1).fit(temp, X[\"Age\"])\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_attribs1 = [\"Name\", \"Cabin\", \"PassengerId\", \"Ticket\"]\n",
    "drop_attribs2 = [\"Name\", \"Cabin\", \"PassengerId\", \"Ticket\", \"Age\"]\n",
    "\n",
    "test_pipeline1 = Pipeline([\n",
    "    ('dropper', AttributeDropper(drop_attribs1)),\n",
    "    ('filler', Filler()),\n",
    "    ('cust_enc', CustEncoder()),\n",
    "    ('family_comb', FamilyCombiner()),\n",
    "    ('hot_encoder', HotEncoder()),\n",
    "    ('age_est', AgeEstimatorTest())\n",
    "])\n",
    "\n",
    "test_pipeline2 = Pipeline([\n",
    "    ('dropper', AttributeDropper(drop_attribs2)),\n",
    "    ('filler', Filler()),\n",
    "    ('cust_enc', CustEncoder()),\n",
    "    ('family_comb', FamilyCombiner()),\n",
    "    ('hot_encoder', HotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = dataset.dropna(subset = [\"Age\"])\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = test_set.sample(frac = 1, axis = 0)\n",
    "sample = shuffle[:820]\n",
    "s_test = shuffle[820:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prep = test_pipeline1.fit_transform(sample)\n",
    "s_test_prep = test_pipeline2.fit_transform(s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = s_test[\"Age\"]\n",
    "age2 = sample[\"Age\"]\n",
    "\n",
    "age_est = test_pipeline1.named_steps['age_est']\n",
    "\n",
    "score = age_est.regr.score(s_test_prep, age)\n",
    "score2 = age_est.regr.score(sample_prep.drop(columns = \"Age\"), age2)\n",
    "print(score, score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X_prep[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined = np.concatenate((train_X_prep, np.vstack(train_y)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = xgb.DMatrix(train_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
